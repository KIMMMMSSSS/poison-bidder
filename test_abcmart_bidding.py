#!/usr/bin/env python3
"""
ABCë§ˆíŠ¸ ì…ì°° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë°ì´í„° ì „ë‹¬ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ í™•ì¸
"""

import json
import logging
from pathlib import Path
from datetime import datetime
from poison_integrated_bidding import AutoBiddingAdapter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_abcmart_bidding():
    """ABCë§ˆíŠ¸ ì…ì°° í…ŒìŠ¤íŠ¸"""
    
    # 1. ABCë§ˆíŠ¸ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë¡œë“œ
    logger.info("=== ABCë§ˆíŠ¸ ì…ì°° í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    json_file = Path("abcmart_products_20250619_223305.json")
    if not json_file.exists():
        logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file}")
        return
    
    with open(json_file, 'r', encoding='utf-8') as f:
        scraped_data = json.load(f)
    
    logger.info(f"ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ: {len(scraped_data)}ê°œ ìƒí’ˆ")
    
    # 2. í…ŒìŠ¤íŠ¸ìš© ì•„ì´í…œ ì¤€ë¹„ (ì²˜ìŒ 3ê°œë§Œ)
    test_items = []
    for product in scraped_data[:3]:  # ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸
        for size_info in product.get('sizes_prices', []):
            if size_info['size'] != 'í’ˆì ˆ':
                test_items.append({
                    'brand': product['brand'],
                    'code': product['product_code'],
                    'product_code': product['product_code'],
                    'color': product.get('color', ''),
                    'size': size_info['size'],
                    'price': size_info['price'],
                    'adjusted_price': int(size_info['price'] * 0.9),  # 10% í• ì¸
                    'link': product['url']
                })
                break  # ê° ìƒí’ˆë‹¹ í•˜ë‚˜ì˜ ì‚¬ì´ì¦ˆë§Œ
    
    logger.info(f"í…ŒìŠ¤íŠ¸ ì•„ì´í…œ ì¤€ë¹„: {len(test_items)}ê°œ")
    if test_items:
        logger.info(f"ì²« ë²ˆì§¸ ì•„ì´í…œ: {json.dumps(test_items[0], ensure_ascii=False, indent=2)}")
    
    # 3. AutoBiddingAdapter í…ŒìŠ¤íŠ¸
    try:
        logger.info("\n=== AutoBiddingAdapter í…ŒìŠ¤íŠ¸ ===")
        adapter = AutoBiddingAdapter(
            driver_path=None,
            min_profit=0,
            worker_count=1  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 1ê°œë§Œ
        )
        
        result = adapter.run_with_poison(test_items)
        
        logger.info(f"ì‹¤í–‰ ê²°ê³¼: {json.dumps(result, ensure_ascii=False, indent=2)}")
        
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    test_abcmart_bidding()


def test_extract_abcmart_links_pagination():
    """ABCë§ˆíŠ¸ ë§í¬ ì¶”ì¶œ í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸"""
    logger.info("\n=== ABCë§ˆíŠ¸ ë§í¬ ì¶”ì¶œ í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        from poison_bidder_wrapper_v2 import PoizonBidderWrapperV2
        import time
        
        # 1. PoizonBidderWrapperV2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        wrapper = PoizonBidderWrapperV2()
        
        # 2. ì‹¤ì œ ê²€ìƒ‰ì–´ë¡œ ë§í¬ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        search_keyword = "ë‚˜ì´í‚¤"  # í…ŒìŠ¤íŠ¸ìš© ê²€ìƒ‰ì–´
        max_pages = 3  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3í˜ì´ì§€ë§Œ
        
        logger.info(f"ê²€ìƒ‰ì–´: '{search_keyword}', ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        
        start_time = time.time()
        links = wrapper.extract_abcmart_links(
            search_keyword=search_keyword,
            max_pages=max_pages
        )
        extraction_time = time.time() - start_time
        
        # 3. ê²°ê³¼ ê²€ì¦
        logger.info(f"\n=== ì¶”ì¶œ ê²°ê³¼ ===")
        logger.info(f"ì¶”ì¶œëœ ë§í¬ ìˆ˜: {len(links)}")
        logger.info(f"ì†Œìš” ì‹œê°„: {extraction_time:.2f}ì´ˆ")
        
        # ë§í¬ê°€ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert len(links) > 0, "ë§í¬ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        logger.info("âœ“ ë§í¬ ì¶”ì¶œ ì„±ê³µ")
        
        # ì¤‘ë³µ ë§í¬ê°€ ì—†ëŠ”ì§€ í™•ì¸
        unique_links = set(links)
        assert len(unique_links) == len(links), f"ì¤‘ë³µ ë§í¬ ë°œê²¬: {len(links) - len(unique_links)}ê°œ"
        logger.info("âœ“ ì¤‘ë³µ ë§í¬ ì—†ìŒ")
        
        # ë§í¬ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
        for link in links[:5]:  # ì²˜ìŒ 5ê°œë§Œ í™•ì¸
            assert link.startswith("https://abcmart.a-rt.com/product?prdtNo="), f"ì˜ëª»ëœ ë§í¬ í˜•ì‹: {link}"
        logger.info("âœ“ ë§í¬ í˜•ì‹ ì •ìƒ")
        
        # ì²˜ìŒ ëª‡ ê°œ ë§í¬ ì¶œë ¥
        logger.info(f"\nì²˜ìŒ 5ê°œ ë§í¬:")
        for i, link in enumerate(links[:5], 1):
            logger.info(f"{i}. {link}")
        
        # 4. íŒŒì¼ ì €ì¥ í™•ì¸
        output_dir = Path("C:/poison_final/logs")
        json_files = list(output_dir.glob(f"abcmart_links_{search_keyword}_*.txt"))
        
        if json_files:
            latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"\níŒŒì¼ ì €ì¥ í™•ì¸: {latest_file}")
            
            # íŒŒì¼ ë‚´ìš© í™•ì¸
            with open(latest_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                file_link_count = sum(1 for line in lines if line.strip().startswith("https://"))
                assert file_link_count == len(links), f"íŒŒì¼ì˜ ë§í¬ ìˆ˜({file_link_count})ì™€ ë°˜í™˜ëœ ë§í¬ ìˆ˜({len(links)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤"
                logger.info("âœ“ íŒŒì¼ ì €ì¥ ì •ìƒ")
        
        logger.info("\n=== í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸ í†µê³¼ ===")
        return True
        
    except ImportError as e:
        logger.error(f"Import ì˜¤ë¥˜: {e}")
        return False
    except AssertionError as e:
        logger.error(f"ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def test_extract_abcmart_links_error_handling():
    """ABCë§ˆíŠ¸ ë§í¬ ì¶”ì¶œ ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    logger.info("\n=== ABCë§ˆíŠ¸ ë§í¬ ì¶”ì¶œ ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        from poison_bidder_wrapper_v2 import PoizonBidderWrapperV2
        
        wrapper = PoizonBidderWrapperV2()
        
        # 1. ë¹ˆ ê²€ìƒ‰ì–´ í…ŒìŠ¤íŠ¸
        logger.info("\n[í…ŒìŠ¤íŠ¸ 1] ë¹ˆ ê²€ìƒ‰ì–´")
        links = wrapper.extract_abcmart_links(search_keyword="", max_pages=1)
        assert len(links) == 0, "ë¹ˆ ê²€ìƒ‰ì–´ì—ì„œ ë§í¬ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤"
        logger.info("âœ“ ë¹ˆ ê²€ìƒ‰ì–´ ì²˜ë¦¬ ì •ìƒ")
        
        # 2. íŠ¹ìˆ˜ë¬¸ì ê²€ìƒ‰ì–´ í…ŒìŠ¤íŠ¸
        logger.info("\n[í…ŒìŠ¤íŠ¸ 2] íŠ¹ìˆ˜ë¬¸ì ê²€ìƒ‰ì–´")
        links = wrapper.extract_abcmart_links(search_keyword="@#$%", max_pages=1)
        # ê²°ê³¼ê°€ ì—†ì–´ë„ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨
        logger.info(f"íŠ¹ìˆ˜ë¬¸ì ê²€ìƒ‰ ê²°ê³¼: {len(links)}ê°œ (ì˜¤ë¥˜ ì—†ìŒ)")
        logger.info("âœ“ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ì •ìƒ")
        
        # 3. ë§¤ìš° ê¸´ ê²€ìƒ‰ì–´ í…ŒìŠ¤íŠ¸
        logger.info("\n[í…ŒìŠ¤íŠ¸ 3] ê¸´ ê²€ìƒ‰ì–´")
        long_keyword = "ë‚˜ì´í‚¤" * 50  # ë§¤ìš° ê¸´ ê²€ìƒ‰ì–´
        links = wrapper.extract_abcmart_links(search_keyword=long_keyword, max_pages=1)
        # ì˜¤ë¥˜ ì—†ì´ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
        logger.info("âœ“ ê¸´ ê²€ìƒ‰ì–´ ì²˜ë¦¬ ì •ìƒ")
        
        logger.info("\n=== ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼ ===")
        return True
        
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def test_unified_bidding_web_scraping():
    """UnifiedBidding ì›¹ ìŠ¤í¬ë˜í•‘ í†µí•© í…ŒìŠ¤íŠ¸"""
    logger.info("\n=== UnifiedBidding ì›¹ ìŠ¤í¬ë˜í•‘ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        from unified_bidding import UnifiedBidding
        import json
        
        # 1. UnifiedBidding ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        bidder = UnifiedBidding(debug=True)
        
        # 2. ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“œë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë§í¬ ì¶”ì¶œë§Œ)
        logger.info("\nì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“œë¡œ ë§í¬ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
        
        # _extract_links ë©”ì„œë“œë§Œ í…ŒìŠ¤íŠ¸
        links = bidder._extract_links(
            site="abcmart",
            web_scraping=True,
            search_keyword="ì•„ë””ë‹¤ìŠ¤"
        )
        
        # 3. ê²°ê³¼ ê²€ì¦
        logger.info(f"ì¶”ì¶œëœ ë§í¬ ìˆ˜: {len(links)}")
        assert len(links) > 0, "ë§í¬ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        logger.info("âœ“ ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“œ ë§í¬ ì¶”ì¶œ ì„±ê³µ")
        
        # 4. JSON íŒŒì¼ ì €ì¥ í™•ì¸
        output_dir = Path("output")
        json_files = list(output_dir.glob("abcmart_links_ì•„ë””ë‹¤ìŠ¤_*.json"))
        
        if json_files:
            latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"\nJSON íŒŒì¼ í™•ì¸: {latest_file}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                saved_data = json.load(f)
                
            assert saved_data['site'] == "abcmart", "ì‚¬ì´íŠ¸ ì •ë³´ ë¶ˆì¼ì¹˜"
            assert saved_data['search_keyword'] == "ì•„ë””ë‹¤ìŠ¤", "ê²€ìƒ‰ì–´ ì •ë³´ ë¶ˆì¼ì¹˜"
            assert saved_data['total_count'] == len(links), "ë§í¬ ìˆ˜ ë¶ˆì¼ì¹˜"
            logger.info("âœ“ JSON íŒŒì¼ ì €ì¥ ë° í˜•ì‹ ì •ìƒ")
        
        # 5. íŒŒì¼ ì½ê¸° ëª¨ë“œ í…ŒìŠ¤íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)
        logger.info("\níŒŒì¼ ì½ê¸° ëª¨ë“œ í…ŒìŠ¤íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)")
        file_links = bidder._extract_links(
            site="abcmart",
            web_scraping=False  # ê¸°ë³¸ê°’
        )
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë°˜í™˜
        logger.info(f"íŒŒì¼ ì½ê¸° ëª¨ë“œ ê²°ê³¼: {len(file_links)}ê°œ")
        logger.info("âœ“ íŒŒì¼ ì½ê¸° ëª¨ë“œ ì •ìƒ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)")
        
        logger.info("\n=== í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼ ===")
        return True
        
    except ImportError as e:
        logger.error(f"Import ì˜¤ë¥˜: {e}")
        return False
    except AssertionError as e:
        logger.error(f"ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False
    except Exception as e:
        logger.error(f"í†µí•© í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def run_all_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("=" * 60)
    logger.info("ABCë§ˆíŠ¸ í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰")
    logger.info("=" * 60)
    
    test_results = []
    
    # 1. ê¸°ì¡´ í…ŒìŠ¤íŠ¸
    logger.info("\n[1/4] ê¸°ì¡´ ABCë§ˆíŠ¸ ì…ì°° í…ŒìŠ¤íŠ¸")
    try:
        test_abcmart_bidding()
        test_results.append(("ê¸°ì¡´ ì…ì°° í…ŒìŠ¤íŠ¸", True))
    except Exception as e:
        test_results.append(("ê¸°ì¡´ ì…ì°° í…ŒìŠ¤íŠ¸", False))
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 2. í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸
    logger.info("\n[2/4] í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸")
    result = test_extract_abcmart_links_pagination()
    test_results.append(("í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸", result))
    
    # 3. ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    logger.info("\n[3/4] ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    result = test_extract_abcmart_links_error_handling()
    test_results.append(("ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸", result))
    
    # 4. í†µí•© í…ŒìŠ¤íŠ¸
    logger.info("\n[4/4] UnifiedBidding í†µí•© í…ŒìŠ¤íŠ¸")
    result = test_unified_bidding_web_scraping()
    test_results.append(("í†µí•© í…ŒìŠ¤íŠ¸", result))
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "=" * 60)
    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info("=" * 60)
    
    passed = 0
    failed = 0
    
    for test_name, result in test_results:
        status = "í†µê³¼" if result else "ì‹¤íŒ¨"
        symbol = "âœ“" if result else "âœ—"
        logger.info(f"{symbol} {test_name}: {status}")
        if result:
            passed += 1
        else:
            failed += 1
    
    logger.info(f"\nì´ {len(test_results)}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {passed}ê°œ í†µê³¼, {failed}ê°œ ì‹¤íŒ¨")
    
    if failed == 0:
        logger.info("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")
    else:
        logger.warning(f"\nâš ï¸  {failed}ê°œì˜ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    return failed == 0


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë¶€ë¶„ ìˆ˜ì •
if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ í™•ì¸
    if len(sys.argv) > 1:
        if sys.argv[1] == "--all":
            # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            success = run_all_tests()
            sys.exit(0 if success else 1)
        elif sys.argv[1] == "--pagination":
            # í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸ë§Œ
            test_extract_abcmart_links_pagination()
        elif sys.argv[1] == "--error":
            # ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ë§Œ
            test_extract_abcmart_links_error_handling()
        elif sys.argv[1] == "--integration":
            # í†µí•© í…ŒìŠ¤íŠ¸ë§Œ
            test_unified_bidding_web_scraping()
        else:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
            test_abcmart_bidding()
    else:
        # ì¸ìˆ˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_abcmart_bidding()
